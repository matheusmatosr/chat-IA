## ğŸ¤– Chat IA: Interface de conversaÃ§Ã£o com modelo Qwen2

Chat interativo com inteligÃªncia artificial capaz de:

- ConversaÃ§Ã£o em tempo real com modelo LLM
- HistÃ³rico de conversas persistente
- Copiar respostas da IA
- VisualizaÃ§Ã£o do status de conexÃ£o
- ConfiguraÃ§Ã£o de parÃ¢metros do modelo

ğŸš€ Tecnologias Utilizadas

- Frontend: React + Material-UI
- Backend: Node.js + Express
- IA: Ollama com modelo Qwen2:7b
- ContainerizaÃ§Ã£o: Docker

ğŸ–¥ï¸ PrÃ©-requisitos

- Docker instalado
- Node.js (v18+)
- NPM

### âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

ConfiguraÃ§Ã£o das variÃ¡veis de ambiente

- Backend (backend/.env):

```bash
PORT=5000
OLLAMA_MODEL=qwen2:7b
OLLAMA_HOST=http://localhost:11434
```

- Frontend (frontend/.env):

```bash
REACT_APP_API_URL=http://localhost:5000
```

### ğŸ–¥ï¸ InstalaÃ§Ã£o

1. Clone este repositÃ³rio

```bash
git clone https://github.com/matheusmatosr/chat-ia.git
```

### âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

1. Iniciar o Ollama (Docker)

```bash
docker-compose up ollama
```

2. Baixar o modelo de IA

```bash
docker exec -it ollama ollama pull qwen2:7b
```

3. Verifique se o modelo foi baixado

```bash
docker exec -it ollama ollama list
```

### Rodar projeto:

#### Backend

Abra o terminal e faÃ§a os seguintes comandos:

1. Para acessar a pasta

```bash
cd backend
```

2. Para instalar as dependÃªncias

```bash
npm install
```

3. Para rodar o backend:

```bash
npm run dev
```

#### Frontend

Abra um novo terminal e faÃ§a os seguintes comandos:

1. Para acessar a pasta

```bash
cd frontend
```

2. Para instalar as dependÃªncias

```bash
npm install
```

3. Para rodar o projeto:

```bash
npm start
```

### ğŸŒ Acesso
O sistema estarÃ¡ disponÃ­vel em:

- Frontend: http://localhost:3000
- Backend API: http://localhost:5000
- Ollama: http://localhost:11434
